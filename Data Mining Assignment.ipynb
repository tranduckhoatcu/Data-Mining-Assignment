{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "from bs4 import BeautifulSoup\n",
    "import ast\n",
    "from urllib.request import Request, urlopen\n",
    "from itertools import combinations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get header from .name file and add to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('house-votes-84.data',header=None)\n",
    "soup_link2 = BeautifulSoup(open('house-votes-84.names'))\n",
    "table_header = soup_link2.find('p')\n",
    "\n",
    "def before(value, a):\n",
    "    pos_a = value.find(a)\n",
    "    if pos_a == -1: return \"\"\n",
    "    return value[0:pos_a]\n",
    "\n",
    "def after(value, a):\n",
    "    pos_a = value.rfind(a)\n",
    "    if pos_a == -1: return \"\"\n",
    "    adjusted_pos_a = pos_a + len(a)\n",
    "    if adjusted_pos_a >= len(value): return \"\"\n",
    "    return value[adjusted_pos_a:]\n",
    "\n",
    "header = after(table_header.text,\"7. Attribute Information:\\n\")\n",
    "header = before(header,\"\\n8. Missing Attribute Values: Denoted by\")\n",
    "list_header = []\n",
    "for i in range(header.count('\\n')):\n",
    "    if(i<10):\n",
    "        buffer = after(header,\" \"+str(i+1)+\". \")\n",
    "    else:\n",
    "        buffer = after(header,str(i+1)+\". \")\n",
    "    buffer = before(buffer,\":\")\n",
    "    list_header.append(str(buffer))\n",
    "\n",
    "dataset.columns = list_header\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are 434 rows and 17 columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.describe(include = 'object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the table above, we can see the basic data structure of dataset: \n",
    "#### All columns is categorical data\n",
    "#### First column \"republican\" have 2 unique value which is (republican and democrat)\n",
    "#### 16 other columns have 3 unique value which are y, n and ? (while ? is the missing value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values is fill with \"?\" in the dataset so we have to take care of it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## replace \"?\" value to most frequent values in each columns\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values = \"?\" , strategy = 'most_frequent',verbose=0)\n",
    "imputer = imputer.fit(dataset.iloc[:,1:])\n",
    "dataset.iloc[:, 1:] = imputer.transform(dataset.iloc[:, 1:])\n",
    "display(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So that all the missing value is fill with the most frequent values\n",
    "#### Because there only 2 unique values for all columns so we don't need dummy variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Correlation Matrix of dataset to check correlations among columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because all columns of the dataset have categorical values so that we’re can not simply use corr() function of Pandas dataframe. we're looking for other measure of association between two categorical features.\n",
    "#### By using Cramér’s V correlation which based on a nominal variation of Pearson’s Chi-Square Test will help us to handle this scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cramers_v(confusion_matrix):\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))\n",
    "\n",
    "cols = list(dataset.columns)\n",
    "corrM = np.zeros((len(cols),len(cols)))\n",
    "for col1, col2 in combinations(cols, 2):\n",
    "    idx1, idx2 = cols.index(col1), cols.index(col2)\n",
    "    corrM[idx1, idx2] = cramers_v(pd.crosstab(dataset[col1], dataset[col2]))\n",
    "    corrM[idx2, idx1] = corrM[idx1, idx2]\n",
    "\n",
    "corr = pd.DataFrame(corrM, index=cols, columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Correlation Matrix of dataset by heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2)\n",
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "ax = sns.heatmap(corr, annot=True, ax=ax); ax.set_title(\"Cramer V Correlation between Variables\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting features based on correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We compare the correlation between features and remove one of two features that have a correlation higher than 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i+1, corr.shape[0]):\n",
    "        if corr.iloc[i,j] >= 0.9:\n",
    "            if columns[j]:\n",
    "                columns[j] = False\n",
    "                selected_columns = dataset.columns[columns]\n",
    "                new_dataset = dataset[selected_columns]\n",
    "remove_columns = (list(set(dataset.columns) - set(new_dataset.columns)))\n",
    "print(str(len(remove_columns))+\" column removed which is: \"+str(remove_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now dataset has only those columns with correlation less than 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(new_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate distribution of selected features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We create new distribution dataframe for calculating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = pd.crosstab(index = new_dataset[\"Class Name\"],columns=new_dataset[\"handicapped-infants\"])\n",
    "buffer = buffer.stack()\n",
    "buffer.index = ['_'.join(idx) for idx in buffer.index]\n",
    "buffer.name = \"handicapped-infants\"\n",
    "distribution = pd.DataFrame(buffer)\n",
    "for col in new_dataset.columns:\n",
    "    if (col == \"Class Name\") or (col == \"handicapped-infants\"):\n",
    "        pass\n",
    "    else:\n",
    "        buffer = pd.crosstab(index = new_dataset[\"Class Name\"],columns=new_dataset[col])\n",
    "        buffer = buffer.stack()\n",
    "        buffer.index = ['_'.join(idx) for idx in buffer.index]\n",
    "        distribution[col] = buffer.values \n",
    "display(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the data to visualize their distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using stacked bar to plot distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "distribution.T.plot(kind='bar', stacked=True,figsize=(20,8),label='big')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelBinarizer, OneHotEncoder\n",
    "# Import here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "oe = OneHotEncoder(sparse=False, dtype=int)\n",
    "training_dataset = dataset.copy()\n",
    "dataset_features = training_dataset.drop(columns='Class Name')\n",
    "dataset_classname = training_dataset['Class Name']\n",
    "X = oe.fit_transform(dataset_features)\n",
    "Y = lb.fit_transform(dataset_classname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree C4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz\n",
    "\n",
    "from graphviz import Source\n",
    "\n",
    "dtModel = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "dtModel.fit(X, dataset_classname)\n",
    "\n",
    "tree = export_graphviz(dtModel, out_file=None, class_names=dataset_classname, proportion=True, filled=True)\n",
    "s = Source(tree, format=\"png\")\n",
    "s.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes classifier can be found in sklearn.naive_bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nbModel = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZeroR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ZeroR, the algorithm finds most frequent class name and return that prediction constantly   \n",
    "Because the dataset has 267 'democrat' records and 168 'republican' records  \n",
    "The accuracy of this ZeroR model should be **Acc = (267)/(267+168) ≈ 0.6138**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZeroR classifier (DummyClassifier) can be found in sklearn.dummy packages\n",
    "from sklearn.dummy import DummyClassifier\n",
    "zeroRModel = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "zeroRModel.fit(X, dataset_classname)\n",
    "print(\"ZeroR accuracy:\", zeroRModel.score(X, dataset_classname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def calc_precision_recall_f1(model, x, y, K=10):\n",
    "    skf = KFold(n_splits=K, shuffle=True)\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    for train_index, test_index in skf.split(x, y):\n",
    "        X_train, X_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return precision_scores, recall_scores, f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,f1 = calc_precision_recall_f1(nbModel, X, Y)    #Precicion and Recall for Naive Bayes Classfier\n",
    "np.mean(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
