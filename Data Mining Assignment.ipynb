{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "from bs4 import BeautifulSoup\n",
    "import ast\n",
    "from urllib.request import Request,urlopen\n",
    "from itertools import combinations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get header from .name file and add to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('house-votes-84.data',header=None)\n",
    "soup_link2 = BeautifulSoup(open('house-votes-84.names'))\n",
    "table_header = soup_link2.find('p')\n",
    "\n",
    "def before(value, a):\n",
    "    pos_a = value.find(a)\n",
    "    if pos_a == -1: return \"\"\n",
    "    return value[0:pos_a]\n",
    "\n",
    "def after(value, a):\n",
    "    pos_a = value.rfind(a)\n",
    "    if pos_a == -1: return \"\"\n",
    "    adjusted_pos_a = pos_a + len(a)\n",
    "    if adjusted_pos_a >= len(value): return \"\"\n",
    "    return value[adjusted_pos_a:]\n",
    "\n",
    "header = after(table_header.text,\"7. Attribute Information:\\n\")\n",
    "header = before(header,\"\\n8. Missing Attribute Values: Denoted by\")\n",
    "list_header = []\n",
    "for i in range(header.count('\\n')):\n",
    "    if(i<10):\n",
    "        buffer = after(header,\" \"+str(i+1)+\". \")\n",
    "    else:\n",
    "        buffer = after(header,str(i+1)+\". \")\n",
    "    buffer = before(buffer,\":\")\n",
    "    list_header.append(str(buffer))\n",
    "\n",
    "dataset.columns = list_header\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are 434 rows and 17 columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.describe(include = 'object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the table above, we can see the basic data structure of dataset: \n",
    "#### All columns is categorical data\n",
    "#### First column \"republican\" have 2 unique value which is (republican and democrat)\n",
    "#### 16 other columns have 3 unique value which are y, n and ? (while ? is the missing value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values is fill with \"?\" in the dataset so we have to take care of it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## replace \"?\" value to most frequent values in each columns\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values = \"?\" , strategy = 'most_frequent',verbose=0)\n",
    "imputer = imputer.fit(dataset.iloc[:,1:])\n",
    "dataset.iloc[:, 1:] = imputer.transform(dataset.iloc[:, 1:])\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So that all the missing value is fill with the most frequent values\n",
    "#### Because there only 2 unique values for all columns so we don't need dummy variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Correlation Matrix of dataset to check correlations among columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because all columns of the dataset have categorical values so that we’re can not simply use corr() function of Pandas dataframe. we're looking for other measure of association between two categorical features.\n",
    "#### By using Cramér’s V correlation which based on a nominal variation of Pearson’s Chi-Square Test will help us to handle this scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cramers_v(confusion_matrix):\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))\n",
    "\n",
    "cols = list(dataset.columns)\n",
    "corrM = np.zeros((len(cols),len(cols)))\n",
    "for col1, col2 in combinations(cols, 2):\n",
    "    idx1, idx2 = cols.index(col1), cols.index(col2)\n",
    "    corrM[idx1, idx2] = cramers_v(pd.crosstab(dataset[col1], dataset[col2]))\n",
    "    corrM[idx2, idx1] = corrM[idx1, idx2]\n",
    "\n",
    "corr = pd.DataFrame(corrM, index=cols, columns=cols)\n",
    "display(corr.abs())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Correlation Matrix of dataset by heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2)\n",
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "ax = sns.heatmap(corr, annot=True, ax=ax); ax.set_title(\"Cramer V Correlation between Variables\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting features based on correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We compare the correlation between features and remove one of two features that have a correlation higher than 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i+1, corr.shape[0]):\n",
    "        if corr.iloc[i,j] >= 0.9:\n",
    "            if columns[j]:\n",
    "                columns[j] = False\n",
    "                selected_columns = dataset.columns[columns]\n",
    "                new_dataset = dataset[selected_columns]\n",
    "remove_columns = (list(set(dataset.columns) - set(new_dataset.columns)))\n",
    "print(str(len(remove_columns))+\" column removed which is: \"+str(remove_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now dataset has only those columns with correlation less than 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(new_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate distribution of selected features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We create new distribution dataframe for calculating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = pd.crosstab(index = new_dataset[\"Class Name\"],columns=new_dataset[\"handicapped-infants\"])\n",
    "buffer = buffer.stack()\n",
    "buffer.index = ['_'.join(idx) for idx in buffer.index]\n",
    "buffer.name = \"handicapped-infants\"\n",
    "distribution = pd.DataFrame(buffer)\n",
    "for col in new_dataset.columns:\n",
    "    if (col == \"Class Name\") or (col == \"handicapped-infants\"):\n",
    "        pass\n",
    "    else:\n",
    "        buffer = pd.crosstab(index = new_dataset[\"Class Name\"],columns=new_dataset[col])\n",
    "        buffer = buffer.stack()\n",
    "        buffer.index = ['_'.join(idx) for idx in buffer.index]\n",
    "        distribution[col] = buffer.values \n",
    "display(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the data to visualize their distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using stacked bar to plot distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "distribution.T.plot(kind='bar', stacked=True,figsize=(20,8),label='big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1 ################################################################################################################\n",
    "#### END 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make classifier with SVM\n",
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode and split the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "oe = OneHotEncoder()\n",
    "training_dataset = new_dataset.copy()\n",
    "dataset_features = training_dataset.drop(columns='Class Name')\n",
    "dataset_classname = training_dataset['Class Name']\n",
    "X = oe.fit_transform(dataset_features)\n",
    "Y = le.fit_transform(dataset_classname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': np.arange(1, 10, 0.3), 'gamma': ['scale', 'auto'], 'kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=0, cv=10, n_jobs=-1)\n",
    "grid.fit(X, Y)\n",
    "print(\"Best 10-fold accuracy:\", grid.best_score_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 7\n",
    "#### END 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 8\n",
    "#### END 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 9\n",
    "#### END 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 10\n",
    "#### END 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 11\n",
    "#### END 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 12\n",
    "#### END 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 13\n",
    "#### END 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 14\n",
    "#### END 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 15\n",
    "#### END 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 16 ##############################################################################################################\n",
    "#### END 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 17\n",
    "#### END 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make classifier with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import copy\n",
    "\n",
    "def getEncodeData(DataFrame, Columns):\n",
    "    labelEncoder = LabelEncoder()\n",
    "    encodedData = copy.deepcopy(DataFrame)\n",
    "    for column in Columns:\n",
    "        encodedData[column] = labelEncoder.fit_transform(DataFrame[column])\n",
    "    return encodedData\n",
    "\n",
    "encodedDataSet = getEncodeData(dataset, list_header[1:])\n",
    "\n",
    "x = encodedDataSet[list_header[1:]]  #Seperate data into input attributes and target attribute\n",
    "y = encodedDataSet['Class Name'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gaussianNaiveBayesModel = GaussianNB().fit(x, y.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using 10-fold Cross Validation to test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score with Naive Bayes model:  [0.95454545 0.93181818 0.97727273 0.81818182 0.97727273 0.90909091\n",
      " 0.97727273 0.97674419 0.76190476 0.95238095]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "gau_score = cross_val_score(gaussianNaiveBayesModel, x, y, cv =10)\n",
    "print(\"Average score with Naive Bayes model: \", gau_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "skf = KFold(n_splits=10)\n",
    "\n",
    "def precision_recall(model):\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    for train_index, test_index in skf.split(x, y):    \n",
    "        X_train, X_test = x.iloc[train_index], x.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        y_pred = model.predict(X_test)\n",
    "        precision_scores.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "        recall_scores.append(recall_score(y_test, y_pred, average='weighted')) \n",
    "    print(\"Precision: \")\n",
    "    print(precision_scores)\n",
    "    print(\"Recall: \")\n",
    "    print(recall_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: \n",
      "[0.9786096256684492, 0.9329945416901939, 0.9780058651026393, 0.8683418031244119, 0.9545454545454546, 0.9570661896243291, 0.9534883720930233, 1.0, 0.8567239635995956, 0.9309465524275804]\n",
      "Recall: \n",
      "[0.9772727272727273, 0.9318181818181818, 0.9772727272727273, 0.8636363636363636, 0.9545454545454546, 0.9534883720930233, 0.9534883720930233, 1.0, 0.813953488372093, 0.9302325581395349]\n"
     ]
    }
   ],
   "source": [
    "precision_recall(gaussianNaiveBayesModel)     #Precicion and Recall for Naive Bayes Classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 21\n",
    "#### END 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 22\n",
    "#### END 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 23\n",
    "#### END 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 24\n",
    "#### END 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 25\n",
    "#### END 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 26\n",
    "#### END 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 26\n",
    "#### END 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 27\n",
    "#### END 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 28\n",
    "#### END 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 29\n",
    "#### END 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 30\n",
    "#### END 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 31 ###############################################################################################################\n",
    "#### END 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 32\n",
    "#### END 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 33\n",
    "#### END 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 34\n",
    "#### END 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 35\n",
    "#### END 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 36\n",
    "#### END 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 37\n",
    "#### END 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 38\n",
    "#### END 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 39\n",
    "#### END 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 40\n",
    "#### END 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 41\n",
    "#### END 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 42\n",
    "#### END 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 43\n",
    "#### END 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 44\n",
    "#### END 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 45\n",
    "#### END 45"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
